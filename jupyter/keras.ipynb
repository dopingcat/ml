{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "%matplotlib inline\n",
    "import urllib.request as req\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random, re\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 58s    \n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.2136 - acc: 0.9347    \n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 43s - loss: 0.1013 - acc: 0.9685    \n",
      "Epoch 3/10\n",
      "14080/60000 [======>.......................] - ETA: 34s - loss: 0.0776 - acc: 0.9758"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer = Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train, y_train)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13500 samples, validate on 1500 samples\n",
      "Epoch 1/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.5187 - acc: 0.7924 - val_loss: 0.2731 - val_acc: 0.9093\n",
      "Epoch 2/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.2483 - acc: 0.9005 - val_loss: 0.2220 - val_acc: 0.8887\n",
      "Epoch 3/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1857 - acc: 0.9227 - val_loss: 0.1462 - val_acc: 0.9347\n",
      "Epoch 4/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1632 - acc: 0.9287 - val_loss: 0.1201 - val_acc: 0.9500\n",
      "Epoch 5/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1435 - acc: 0.9387 - val_loss: 0.1103 - val_acc: 0.9520\n",
      "Epoch 6/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1372 - acc: 0.9423 - val_loss: 0.0759 - val_acc: 0.9860\n",
      "Epoch 7/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1305 - acc: 0.9426 - val_loss: 0.1195 - val_acc: 0.9387\n",
      "Epoch 8/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1295 - acc: 0.9451 - val_loss: 0.0748 - val_acc: 0.9740\n",
      "Epoch 9/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1235 - acc: 0.9473 - val_loss: 0.0690 - val_acc: 0.9807\n",
      "Epoch 10/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1219 - acc: 0.9449 - val_loss: 0.3040 - val_acc: 0.8660\n",
      "Epoch 11/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1218 - acc: 0.9458 - val_loss: 0.0614 - val_acc: 0.9840\n",
      "Epoch 12/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1120 - acc: 0.9544 - val_loss: 0.1069 - val_acc: 0.9447\n",
      "Epoch 13/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1111 - acc: 0.9533 - val_loss: 0.0535 - val_acc: 0.9913\n",
      "Epoch 14/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1115 - acc: 0.9519 - val_loss: 0.3136 - val_acc: 0.8707\n",
      "Epoch 15/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1022 - acc: 0.9567 - val_loss: 0.0711 - val_acc: 0.9673\n",
      "Epoch 16/20\n",
      "13500/13500 [==============================] - 1s - loss: 0.1038 - acc: 0.9550 - val_loss: 0.1453 - val_acc: 0.9267\n",
      "4448/4999 [=========================>....] - ETA: 0sloss= 0.144984994898\n",
      "accuracy= 0.926985397079\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "csv = pd.read_csv(\"bmi.csv\")\n",
    "\n",
    "csv[\"weight\"] /= 100\n",
    "csv[\"height\"] /= 200\n",
    "# X = csv[[\"weight\", \"height\"]].as_metrix()\n",
    "\n",
    "bclass = {\"thin\":[1,0,0], \"normal\":[0,1,0], \"fat\":[0,0,1]}\n",
    "y = np.empty((20000, 3))\n",
    "\n",
    "for i,v in enumerate(csv[\"label\"]):\n",
    "    y[i] = bclass[v]\n",
    "    \n",
    "X_train, y_train = X[1:15001], y[1:15001]\n",
    "X_test, y_test = X[15001:20001], y[15001:20001]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(2,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    nb_epoch=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=2)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json\n",
    "\n",
    "root_dir = \"../../origin/news_data\"\n",
    "dic_file = root_dir + \"/word-dic.json\"\n",
    "data_file = root_dir + \"/data.json\"\n",
    "data_file_min = root_dir + \"/data-mini.json\"\n",
    "\n",
    "# 어구를 자르고 ID 로 변환하기\n",
    "word_dic = { \"_MAX\": 0 }\n",
    "def text_to_ids(text):\n",
    "    text = text.strip()\n",
    "    words = text.split(\" \")\n",
    "    result = []\n",
    "    for n in words :\n",
    "        n = n.strip()\n",
    "        if n == \"\": continue\n",
    "        if not n in word_dic : \n",
    "            wid = word_dic[n] = word_dic[\"_MAX\"]\n",
    "            word_dic[\"_MAX\"] += 1\n",
    "            print(wid, n)\n",
    "        else :\n",
    "            wid = word_dic[n]\n",
    "        result.append(wid)\n",
    "    return result\n",
    "\n",
    "# 파일을 읽고 고정 길이의 배열 리턴\n",
    "def file_to_ids(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read()\n",
    "        return text_to_ids(text)\n",
    "    \n",
    "# 딕셔너리에 단어 모두 등록\n",
    "def register_dic():\n",
    "    files = glob.glob(root_dir + \"/*/*.wakati\", recursive=True)\n",
    "    for i in files:\n",
    "        file_to_ids(i)\n",
    "        \n",
    "def count_file_freq(fname):\n",
    "    cnt = [ 0 for n in range(word_dic[\"_MAX\"]) ]\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read().strip()\n",
    "        ids = text_to_ids(text)\n",
    "        for wid in ids:\n",
    "            cnt[wid] += 1\n",
    "    return cnt\n",
    "\n",
    "# 카테고리마다 파일 읽어 들이기\n",
    "def count_freq(limit = 0):\n",
    "    X = []\n",
    "    Y = []\n",
    "    max_words = word_dic[\"_MAX\"]\n",
    "    cat_names = []\n",
    "    for cat in os.listdir(root_dir):\n",
    "        cat_dir = root_dir + \"/\" + cat\n",
    "        if not os.path.isdir(cat_dir): continue\n",
    "        cat_idx = len(cat_names)\n",
    "        cat_names.append(cat)\n",
    "        files = glob.glob(cat_dir+\"/*.wakati\")\n",
    "        i = 0\n",
    "        for path in files:\n",
    "            print(path)\n",
    "            cnt = count_file_freq(path)\n",
    "            X.append(cnt)\n",
    "            Y.append(cat_idx)\n",
    "            if limit > 0:\n",
    "                if i > limit: break\n",
    "                i += 1\n",
    "    return X, Y\n",
    "\n",
    "# 단어 딕셔너리 만들기\n",
    "if os.path.exists(dic_file) :\n",
    "    word_dic = json.load(open(dic_file))\n",
    "else :\n",
    "    register_dic()\n",
    "    json.dump(word_dic, open(dic_file, \"w\"))\n",
    "    \n",
    "# 벡터를 파일로 출력\n",
    "# 테스트 목적의 소규모 데이터 만들기\n",
    "X, Y = count_freq(20)\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file_min, \"w\"))\n",
    "# 전체 데이터를 기반으로 데이터 만들기\n",
    "X, Y = count_freq()\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file, \"w\"))\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
